---
title: "Sorry, I don't like AI"
author: kai
date: 2025-07-07
slug: []
categories: [personal]
tags:
  -blog
format: hugo-md
description: "Just what the internet needed: Another blog post about not liking AI"
---


<a id="orgb860595"></a>

# Introduction

Yes, another blog post taking a Bold Stance on AI<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>. But this blog post is for *me*, and only incidentally for you. I post it because having the stakes of an audience (however small) helps me. Otherwise I wouldn&rsquo;t poison the world with my drivel.


<a id="orgefc6eee"></a>

## Why am I writing this?

The use of AI has made me feel increasingly uncomfortable, sad, and angry, and I wanted to disentangle and understand those source of those feelings. Despite what *some* folks will say, feelings are crucial to the collection and assembly of facts, and in a healthy scenario work in a virtuous cycle: Feelings direct and prioritize the collection of facts, while facts can regularize or affirm our feelings. This is what we call &rsquo;thinking&rsquo;. So I&rsquo;m here because some thinking needs to be done.

Intended outcomes:

1.  Clarity. Writing has a tendency to smooth out the swiss-cheese logic of pure thought.
2.  Regularization or vindication of feelings. This serves as a place to organize my thoughts and do some light research.

Not indented outcomes:

1.  Convincing anyone. Even if that was my intent, I think changing someone&rsquo;s mind of something they are diametrically opposed to is not possible in even the best blog post.
2.  Making you feel bad. This is, at worst, venting &#x2013; not punitive.
3.  Produce a definitive and/or objective answer. The issues discussed here tie in with issues as big as &rsquo;being alive&rsquo;. I don&rsquo;t think an objective answer exists and regardless we aren&rsquo;t going to crack it here.

Let&rsquo;s dive in.


<a id="org0385552"></a>

# I don&rsquo;t like what AI does


<a id="org4d2fb4d"></a>

## I don&rsquo;t like the quality of AI output

I think the most straightforward argument to be made against AI is the accuracy of its statements. Many of my friends use them and seem to have some success with them, and I believe that by using (1) properly tuned models of (2) sufficient size on (3) problems they have a large amount of data for&#x2026;they can do ok.

Regardless of suitability, models are pretty bad at throwing in the towel when they should. If I were to ask a problem to a human, they might say &ldquo;I don&rsquo;t know&rdquo; if they don&rsquo;t have enough information, or they might give vague pointers on where to find the answer, or if they&rsquo;re *really* afraid of not knowing the answer, they&rsquo;ll give a mealymouthed lie that will fold under some cross-validation. What they *won&rsquo;t* do is rattle off an incredibly plausible sounding, detailed, but wrong answer that makes finding the issue to the answer a game of Where&rsquo;s Waldo (and in the case of [slopsquatting](https://en.wikipedia.org/wiki/Slopsquatting), sometimes Waldo has a *bomb*)<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>.

Yes, it&rsquo;s always worthwhile to do a gut check, consider the source, and decide if some information should be cross-validated. But these lies &#x2013; excuse me, hallucinations &#x2013; are superhuman in their presentation: the models have been fed every book, blog post, and codebase they could (legally, ethically, or otherwise) obtain. They are *masters* of producing &rsquo;sentence shaped&rsquo; things, and, by that nature, sometimes those things happen to be true (For example, &rsquo;dsfajio bteoqhhu vsuqeiq&rsquo; is less likely to be true than &rsquo;orange happy table octopus&rsquo; is less likely to be true than &rsquo;I am a small dog&rsquo;). And indeed, through ingesting a lot of information, it&rsquo;s often correct.

However, I just asked &ldquo;What does they saying &lsquo;No dogs but the Sunday best&rsquo; mean?&rdquo; and it gave me four paragraphs AND offered to tell me about the origins of the phrase, which was followed by four more paragraphs of horse shit. No worries here: I *know* it&rsquo;s nonsense. But what if I had an actual question, with an unknown truth value? Bruno Dias wrote a beautiful (short!) post about how this &rsquo;right sounding&rsquo; aspect of LLMs make it &#x2013; arguably &#x2013; not useful. You should read it right now, even if it means not reading the rest of this blog post: [link to Bruno&rsquo;s post](https://azhdarchid.com/are-llms-useful/).


<a id="org6626032"></a>

## I don&rsquo;t like the social side effects that the output creates

I honestly think that the accuracy of LLMs isn&rsquo;t its biggest issue. In isolation, I would just not interact with it and call it a day. I use plenty of imperfect tech, and I also opt out of plenty of tech I don&rsquo;t like, and I don&rsquo;t write a whole blog post about all of them. The problem is that LLMs and generative models are a kind of &ldquo;Oops! All Negative Externalities&rdquo;.

Something that I&rsquo;m sure you&rsquo;ve noticed is that *the stuff is everywhere*, like a fine silt spread across the internet. That&rsquo;s not even to speak of the *output* of the models, just the implementation of and advertisement of various AI chatbots. There is a strong incentive for usage to be normalized and adopted, in large part I&rsquo;m sure because these companies are *bleeding money*<sup><a id="fnr.3" class="footref" href="#fn.3" role="doc-backlink">3</a></sup>. The curious thing about them, though, is that they are at once:

1.  Advertised as replacing any and every imaginable job, and
2.  Should never be trusted and all information verified


<a id="orge1c4879"></a>

### I&rsquo;d like to speak to a human.

When I read something, look at art, listen to music, or even read DMs and emails, I see it as a window to the person or people who created it. Creating things - even something as small as a text message - requires *engagement*. By engaging with whatever you&rsquo;re creating, you&rsquo;re adding a little sample of yourself. As a recipient, I can read that text and know, with a certain amount of assurance, that interpretation of that creating product will yield insight about the person who made it and their intentions and context. It&rsquo;s a fruitful process, or it can be one if I &#x2013; and you &#x2013; put in the effort.

If a *robot* churns out art, I don&rsquo;t really see the point in engaging with it seriously. Creating things forces you to think intimately about minutia you wouldn&rsquo;t even second glance at when &rsquo;overseeing&rsquo; a robot. Why would I scrutinize the details if you aren&rsquo;t in there? And even worse, and email? I get the drudgery of emails, but to have emails read by no one, written by no one, expanding and summarizing in a wasteful, absurd game of robot telephone - is that better? Or at the very least, do you think that&rsquo;s a *solution*? Human connection is precious and increasingly rare, particularly in the age of overly polished, LinkedIn-facade arms race to scramble for jobs in a hostile marketplace. But I don&rsquo;t see expediting it through automation as *helping*.


<a id="org01d7099"></a>

### Struggling *is* learning

It&rsquo;s so, *so* easy to fool yourself into thinking you&rsquo;ve learned something because it went down smooth. Lately, I&rsquo;ve been trying to teach myself linear algebra. Reading about linear algebra? Piece of cake. Doing the exercises? Pure agony. All the information was *technically* in the chapter, but along the way I have made so many false assumptions, missed so many details, that by the end of the chapter it&rsquo;s like I read something entirely different.

For everyday stuff, this surface level &rsquo;yeah I get the gist&rsquo; type thing is *fine*. I don&rsquo;t need to be able to pass a comprehension exam on, say, where all the produce is located at my local grocery store. But for the technical stuff, to be able to *create* with it, I have to wrestle with it. Failure can be immensely useful: it can highlight areas for improvement and reveal false assumptions. And when things are difficult? You&rsquo;re building those neural pathways, baby. Just like working out (I assume), no pain, no gain. And goddamn does it feel good to finally start to feel proficient at something that was previously impossible.

The problem with LLMs is that they make this illusion of learning possible for pretty much anything. I once spoke with someone who said that they had a lot of really good thoughts but ChatGPT just helped them write it down, and I have to say I&rsquo;m not entirely convinced. If I can&rsquo;t accurately convey them, then how could an LLM know what I mean? I don&rsquo;t think the results would even be my own thoughts. At best, my thoughts were chewed up and spit into my mouth (no pain no gain). At worst, it&rsquo;s a [Barnum effect](https://en.wikipedia.org/wiki/Barnum_effect) that has hypnotized me into believing my thoughts were made tangible: a narrative so full of holes and constraints that nearly any text could fill in the details.

Counterpoint: some people just want to be able to do things without learning *how* to do it. Maybe you have no coding experience and you still want to [vibe code](https://en.wikipedia.org/wiki/Vibe_coding) something bespoke. But in general I&rsquo;m unimpressed with that line of reasoning. If it&rsquo;s something big, it&rsquo;s worth engaging with seriously and learning the fundamentals. If it&rsquo;s something silly and whimsical, the fact that it was Robot Art, for the reasons stated above, make it largely devoid of value for me personally.

Further counterpoint: maybe you&rsquo;re working a 9-5 and you just need to ship a feature by a rapidly approaching deadline. You *probably* could write it, but it would take you a lot longer. To which I say&#x2026;whatever. I&rsquo;m not going to make you lay down on the tracks to attempt to stop the barreling train of late-stage capitalism. But as far as I understand, the jury is still out if using LLMs improves efficiency, and certainly now there is code in your codebase that no one *really* understands.

#### A word on students completing assignments with LLMs.

A discussion of learning and LLMs wouldn&rsquo;t be nutritionally balance without piping-hot-takes on LLMs in education.
    
Here&rsquo;s the way I see it: LLM generated content can&rsquo;t be detected programatically<sup><a id="fnr.4" class="footref" href="#fn.4" role="doc-backlink">4</a></sup>. With LLMs, students can effortlessly produce content that &#x2013; at the very least &#x2013; *looks* right. Other students might choose not to use LLMs, which will ultimately take much, much longer, and this is often at the expense of time that could be spent on *other* courses. Through grading, these assignments are converted into GPAs<sup><a id="fnr.5" class="footref" href="#fn.5" role="doc-backlink">5</a></sup>, which are converted into job prospects. And yet again we find ourselves in another hellish arms race akin to that of CV and resumé padding, further degradation of already poorly reliable indicators. GPA always was a poor indicator of this amorphous idea of &rsquo;knowledge&rsquo;, and has become a target rather than an indicator ([Goodhart&rsquo;s Law](https://en.wikipedia.org/wiki/Goodhart%27s_law)), but this feels significantly worse.
    
I&rsquo;ve seen pragmatic professors change their assignments to be more LLM-proof, and I&rsquo;m sympathetic to their needs to get *something* working without having to overhaul the system. Ultimately, I predict that stricter (likely imperfect and certainly irritating) detection and surveillance methods will be put in place to prevent LLM usage, in an attempt to maintain the current &rsquo;GPA infrastructure&rsquo;. And I get it, burning it all down is impractical. I just wonder what life would be like if we embraced a little more chaos rather than clinging desperately to soothing but misleading quantification.


<a id="org27560b1"></a>

### Turbo Bullshit Machine

The *output* from these models is near ubiquitous. You&rsquo;ve already seen it - Facebook/Instagram posts, the &rsquo;chum&rsquo; at the bottom of news articles, Google searches, endless labyrinthine blogs chock-full with nonsensical posts - it&rsquo;s everywhere, and the better you get at identifying it, the more pervasive you realize it is.

At best, they&rsquo;re poor facsimiles of actual content, low effort memes or uninteresting art, adding more noise to an already noisy internet. At worst, they&rsquo;re indistinguishable inflammatory comments spouting views held by *no one* for the sole purpose of creating chaos in an already *extremely* divisive era<sup><a id="fnr.6" class="footref" href="#fn.6" role="doc-backlink">6</a></sup>.

And it&rsquo;s not just the slop these models produce: they inevitably poison the perception of content around them. My willingness to engage with *anything* knowing I&rsquo;m now in a slop swamp decreases precipitously, even if there *is* a diamond in the rough. And maybe you think it&rsquo;s foolish to expect to engage with anything on the internet, to let anything under that cynical carapace - slop or not. But that&rsquo;s a level of pessimism that not even I stoop to: digital media can be beautiful and we should allow it to change us. The fact that these models have essentially created high-throughput bullshit printers, coughing out nonsense-pollution, is, frankly, shameful.


<a id="orgb8ddcce"></a>

# I don&rsquo;t like *how* AI does

I don&rsquo;t just have complaints about the product, but also the method! Really covering all my bases here.


<a id="org6171d1e"></a>

## Extractive

Generative models need gobs and gobs of data, and are currently trained with token numbers approaching the number of words in every single book on Earth. This requires tapping in to pretty much any resource possible. Books, of course, but also blog posts, chats, social media - whatever. And that&rsquo;s just for language models. Image-based models also need astronomical amounts of data too: The first generation of DALL-E has a dataset of about 250 million images ([source](https://arxiv.org/pdf/2204.06125)), though these companies tend to be pretty close to the breast about the size of their training data. These data have to come from *somewhere*, and the ethics about how this happens is an oft talked about, highly contentious, usually subjective, and rapidly evolving subject in the generative AI space.

I read books and look at images all the time, and then I make stuff. For creative work, this is often without credit to the sources I got inspiration from, because it&rsquo;s often an amalgamation of *so many things* and difficult to point to any individual source<sup><a id="fnr.7" class="footref" href="#fn.7" role="doc-backlink">7</a></sup>. This sounds a lot like what a generative AI model does. If it&rsquo;s ok for me to do it, what&rsquo;s the difference if a model does it?

This is a bit of a ham-handed [universalizability](https://en.wikipedia.org/wiki/Universalizability) argument: &rsquo;if it&rsquo;s ok *one* time, it&rsquo;s ok millions of times&rsquo;. If you see no flaws with the argument, I encourage you to never try beer. But even beyond the idea of scale, it&rsquo;s incredibly dispassionate to think that all that art humans do is just an amalgamation of things we&rsquo;ve seen.

At risk of repeating myself from earlier sections, our desires, culture, environment, and even present affect direct what we consume and, in turn, create. So no, I don&rsquo;t think that the purely extractive process of generative models slurping up every image available is the same as what artists do. And, lest you forget, an artist is a person outside of what they produce.


<a id="org9bea740"></a>

## Consent

There is currently a mad rush to mine every last vein of fresh tokens to train these models, and the automated web crawlers that hoover up information are *ruthless*. Theoretically, a \`robots.txt\` file should be a line of salt that tells such crawlers that they aren&rsquo;t welcome there, but for some reason<sup><a id="fnr.8" class="footref" href="#fn.8" role="doc-backlink">8</a></sup> these genAI crawlers don&rsquo;t listen. In fact, they tend to [actively avoid detection](https://drewdevault.com/2025/03/17/2025-03-17-Stop-externalizing-your-costs-on-me.html) during their pillaging. Besides being incredibly rude, these things are crushing servers - [Wikimedia, for instance](https://diff.wikimedia.org/2025/04/01/how-crawlers-impact-the-operations-of-the-wikimedia-projects/) - in a way that makes solutions like [nightshade](https://nightshade.cs.uchicago.edu/whatis.html#), [anubis](https://github.com/TecharoHQ/anubis), and [Cloudflare&rsquo;s labyrinth](https://blog.cloudflare.com/ai-labyrinth/) necessary defense measures in case bots decide they aren&rsquo;t interested in obtaining consent.

On the other end of things, you have pretty much every last tech company absolutely horny to sell your data to train AI. For LinkedIn, even if you previously disabled consent, it was *re-enabled* at a later date. Instagram, Facebook, Tumblr, Microsoft Excel and Word, DeviantArt, [Zillow](https://www.zillowgroup.com/zg-privacy-policy/), hell, [Adobe Acrobat](https://infosec.exchange/@fifonetworks/114804073360311475) - all configured to consume your data by default and feed it to some model.

And hey, maybe that&rsquo;s what you signed up for when you accepted those T&Cs you didn&rsquo;t read, but let&rsquo;s be honest: This isn&rsquo;t opt-in for a reason. There&rsquo;s no fanfare when it comes to announcing that they&rsquo;re using your data to train a model, and all the buttons to disable it are at the bottom of some menu seven layers deep. It&rsquo;s *heavily implied* that they know you aren&rsquo;t going to like this, so they&rsquo;re hoping they can sneak this one by you, or maybe just wear you down.

&rsquo;You don&rsquo;t have to use their service&rsquo; you say, and maybe you&rsquo;re technically right. But in practice? Listen, I certainly don&rsquo;t *want* to use Microsoft Office, or LinkedIn, but you know what I *do* want? A place to live. Artists who need to advertise and sell their work won&rsquo;t get the same amount of reach using a self-hosted solution (not to mention the technical know-how and possibly capital required for such endeavors). These companies, by design, strive to make themselves irreplaceable and have the [network effect](https://en.wikipedia.org/wiki/Network_effect#) on their side. The choice has largely been made for you.


<a id="org82c302b"></a>

## Carbon Footprint

I&rsquo;m still unclear how big of an impact generative model training and usage is having and will have in the future vis-à-vis carbon emissions. As I currently see them as largely devoid of value, *any* significant release of greenhouse gases feels to be too much. Current projections indicate that by 2028, ~7-12% of US energy will datacenters, and a lot of that will be AI related ([source](https://newscenter.lbl.gov/2025/01/15/berkeley-lab-report-evaluates-increase-in-electricity-demand-from-data-centers/)). So that feels pretty bad. Additionally, it&rsquo;s been noted that datacenters tend to use dirtier sources of electricity due to their necessity for high uptime ([source](https://www.technologyreview.com/2025/05/20/1116327/ai-energy-usage-climate-footprint-big-tech/)). So actually yeah by the end of writing this I think I&rsquo;m convinced that energy is a significant problem as well.


<a id="orgaf228bc"></a>

# Conclusion

I thought about writing about how I don&rsquo;t like the projected future of AI &#x2013; that is, the projections given by Sam Altman and the like &#x2013; but frankly I find their posturing more irritating than substantive and I&rsquo;ve had this blog post weighing on me for weeks now and just want to get it off my chest. There&rsquo;s some other stuff I wanted to touch on too, like how my colleagues who speak English as a second language use it to help with their phrasing. Short answer (deep breath): I&rsquo;d rather have the raw version with some mistakes but I understand that might put these people at a disadvantage professionally, and I don&rsquo;t know how to solve that and it seems like a larger cultural issue but I don&rsquo;t know what to do in the meantime.

Bottom line? Don&rsquo;t like genAI models. Sorry. I&rsquo;m sure you&rsquo;re great.


# Footnotes

<sup><a id="fn.1" href="#fnr.1">1</a></sup> By &ldquo;AI&rdquo;, I mean - usually - large language models (like ChatGPT et al.). Sometimes I&rsquo;ll include image-based generative models (Like, say, DALL-E) in that definition. The context will hopefully be obvious.

<sup><a id="fn.2" href="#fnr.2">2</a></sup> ChatGPT (4o-mini) appears to be *pretty* good at asking for clarifying questions when given intentionally confusing phrases in my tests, but it *did* suggest a recipe for a &rsquo;black-bean mint julep&rsquo; when asked, insisting that while non-traditional, the &rsquo;black bean syrup&rsquo; it provided instructions for would provide an &rsquo;earthy&rsquo; flavor.

<sup><a id="fn.3" href="#fnr.3">3</a></sup> <https://www.wheresyoured.at/openai-is-a-systemic-risk-to-the-tech-industry-2/>

<sup><a id="fn.4" href="#fnr.4">4</a></sup> And woe to thee who thinks it can be: <https://social.lol/@von/114788592078507875>

<sup><a id="fn.5" href="#fnr.5">5</a></sup> And it&rsquo;s bonkers that we turn a percentage into a grade back into a number again

<sup><a id="fn.6" href="#fnr.6">6</a></sup> Actually, I consider writing [court documents](https://www.businessinsider.com/increasing-ai-hallucinations-fake-citations-court-records-data-2025-5) and [commission reports](https://www.foodpolitics.com/2025/06/the-maha-commission-report-documented-by-ai-does-it-matter-yes-a-lot-2/) with non-existent citations to be much worse, but whatever.

<sup><a id="fn.7" href="#fnr.7">7</a></sup> This is actually pretty untrue for me. I have specific artists that I admire and follow on social networks and am inspired by them and their writing.

<sup><a id="fn.8" href="#fnr.8">8</a></sup> (money)
