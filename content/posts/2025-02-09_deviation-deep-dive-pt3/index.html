<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
	"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">

<head>
<title>index.html</title>
<meta http-equiv="Content-Type" content="application/xhtml+xml;charset=utf-8"/>
<link rel="stylesheet" type="text/css" media="all" href="https://cdn.jsdelivr.net/npm/github-markdown-css/github-markdown.min.css"  />
<link rel="stylesheet" type="text/css" media="all" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release/build/styles/github.min.css"  /><meta name='viewport' content='width=device-width, initial-scale=1, shrink-to-fit=no'><style> body { box-sizing: border-box; max-width: 740px; width: 100%; margin: 40px auto; padding: 0 10px; } </style><script id='MathJax-script' async src='https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'></script><script src='https://cdn.jsdelivr.net/gh/highlightjs/cdn-release/build/highlight.min.js'></script><script>document.addEventListener('DOMContentLoaded', () => { document.body.classList.add('markdown-body'); document.querySelectorAll('pre[lang] > code').forEach((code) => { code.classList.add(code.parentElement.lang); }); document.querySelectorAll('pre > code').forEach((code) => { hljs.highlightBlock(code); }); });</script>
</head>

<body>

<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kaimeleon)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>ACCENT <span class="ot">&lt;-</span> <span class="st">&quot;#f7d060&quot;</span></span></code></pre></div>
<h1 id="introduction">Introduction</h1>
<p>In previous blog posts [<a
href="https://kai.rbind.io/posts/2024-09-22_deviation-deep-dive-pt1/">1</a>]
[<a
href="https://kai.rbind.io/posts/2025-01-06_deviation-deep-dive-pt2/">2</a>],
I’ve alluded to the fact that when we calculate the standard deviation,
we sometimes divide by <span class="math inline">\(n - 1\)</span> (that
is, one less than the number of samples we have) rather than <span
class="math inline">\(n\)</span>. This is known as ‘Bessel’s
correction’, and it’s typically sneaked in after learning how to
calculate the standard deviation, hopefully quickly enough so you won’t
notice. However, I think it’s fairly common to ask why we do this, and
the standard response is usually a mumbling answer along the lines of it
being a correction factor, and that you should only do it if you’re
calculating the population’s standard deviation rather than the sample’s
standard deviation, turning this corrective factor not only into a
mysterious oddity, but now also part of a shadowy ritual. My thoughts to
this answer were typically ‘correction for what?’ and ‘wow that seems
pretty haphazard’, and usually resulted in me always dividing by <span
class="math inline">\(n-1\)</span> and hoping nothing bad would
happen.</p>
<p>During my quest for enlightenment, I came across several different
explanations, with varying levels of detail and coherence, which I’ll
explain back to you in order of increasing detail. I very much regret
this endeavor.</p>
<h1 id="conventions">Conventions</h1>
<p>Unfortunately, this blog post is going to contain some math. Instead
of breaking up proofs by saying ‘and we can do this step because of XYZ’
which can make following along difficult, I’m doing to include symbols
above equals signs that will direct you to a section in the appendix
that should show you why this is possible. For instance:</p>
<p><span class="math display">\[\begin{align*}
E[A + B] \overset{1}= E[A] + E[B]
\end{align*}\]</span></p>
<p>Where 1 corresponds to the number in the appendix.</p>
<p>Also, I tried my best to use colors in equations to draw your
attention to the changing parts. Apologies if it’s distracting. I find
my eyes glaze over pretty quickly if they don’t have ‘landmarks’.</p>
<p>Finally, I’ve chosen to collapse code that I didn’t find pertinent to
the lesson at hand, but feel free to expand it if you’re interested!</p>
<h1 id="explanation-i-it-just-works-ok">Explanation I: It just works,
OK?</h1>
<p>The first and easiest way to look at this is to simply <em>not</em>
look at it, a tactic similar to the ‘it’s working don’t touch it’
strategy I take with particularly arcane sections of code. So maybe we
should kick the tires and make sure this thing is even working in the
first place. It certainly would be convenient if we didn’t actually need
this thing in the first place.</p>
<p>Let’s simulate some samples:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">13</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>replicates <span class="ot">&lt;-</span> <span class="dv">100000</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>sd <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>samples <span class="ot">&lt;-</span> <span class="fu">replicate</span>(replicates, <span class="fu">rnorm</span>(n, <span class="at">sd =</span> sd), <span class="at">simplify =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>And let’s plot some samples to wrap our head around them:</p>
<details>
<summary>
Plotting code
</summary>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>some <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>plotting_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">value =</span> <span class="fu">unlist</span>(samples[<span class="fu">seq_len</span>(some)]),</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">replicate =</span> <span class="fu">factor</span>(<span class="fu">rep</span>(<span class="fu">seq_len</span>(some), <span class="at">each =</span> n))</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(plotting_data, <span class="fu">aes</span>(value, replicate, <span class="at">color =</span> <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>, <span class="at">color =</span> <span class="st">&quot;white&quot;</span>, <span class="at">linewidth =</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">shape =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_kai</span>(<span class="st">&quot;dark&quot;</span>) <span class="sc">+</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">legend.position =</span> <span class="st">&quot;none&quot;</span>,</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">axis.text.y =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">panel.grid.minor =</span> <span class="fu">element_blank</span>()</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="cn">NULL</span>, <span class="at">y =</span> <span class="st">&quot;Replicate&quot;</span>)</span></code></pre></div>
</details>
<p><img style="width:20%;" src="samples.png" /></p>
<p>Each replicate has 10 values in it, sampled from a normal
distribution with a mean of 0 and a standard deviation of 10.</p>
<p>However, just because it was <em>sampled</em> from a distribution
with those parameters doesn’t mean that the <em>samples themselves</em>
will have those parameters on average, so let’s check it out:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We&#39;re creating our own functions because R&#39;s automatically applies Bessel&#39;s</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># correction</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>calc_var <span class="ot">&lt;-</span> <span class="cf">function</span>(values, <span class="at">center =</span> <span class="fu">mean</span>(values)) {</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">length</span>(values)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  sq_diffs <span class="ot">&lt;-</span> (values <span class="sc">-</span> center)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sum</span>(sq_diffs<span class="sc">/</span>n)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>calc_sd <span class="ot">&lt;-</span> <span class="cf">function</span>(values, <span class="at">center =</span> <span class="fu">mean</span>(values)) {</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sqrt</span>(<span class="fu">calc_var</span>(values, center))</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>sds <span class="ot">&lt;-</span> <span class="fu">sapply</span>(samples, calc_sd)</span></code></pre></div>
<details>
<summary>
Plotting code
</summary>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">x =</span> sds), <span class="fu">aes</span>(x)) <span class="sc">+</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_kai</span>() <span class="sc">+</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">bins =</span> <span class="dv">1000</span>) <span class="sc">+</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> sd, <span class="at">color =</span> ACCENT) <span class="sc">+</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fu">mean</span>(sds), <span class="at">color =</span> ACCENT, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Sample SD&quot;</span>, <span class="at">y =</span> <span class="cn">NULL</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="fu">ggsave</span>(<span class="st">&quot;sds.png&quot;</span>, plot, <span class="at">dpi =</span> <span class="dv">300</span>, <span class="at">width =</span> <span class="dv">4</span>, <span class="at">height =</span> <span class="dv">4</span>, <span class="at">units =</span> <span class="st">&quot;in&quot;</span>)</span></code></pre></div>
</details>
<p><img style="width:50%;" src="sds.png" /></p>
<p>In this figure I’m showing the distribution of the calculated
standard deviations (without correction!!) for all 100,000 samples. The
dotted line represents the mean of the distribution, while the solid
line represents the true population standard deviation. As you can tell,
it’s being a bit underestimated.</p>
<p>What about if we use Bessel’s correction?</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Using R&#39;s built-in sd function, which applies Bessel&#39;s correction</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>sds <span class="ot">&lt;-</span> <span class="fu">sapply</span>(samples, sd)</span></code></pre></div>
<details>
<summary>
Plotting code
</summary>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">x =</span> sds), <span class="fu">aes</span>(x)) <span class="sc">+</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_kai</span>() <span class="sc">+</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">bins =</span> <span class="dv">1000</span>) <span class="sc">+</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> sd, <span class="at">color =</span> ACCENT) <span class="sc">+</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fu">mean</span>(sds), <span class="at">color =</span> ACCENT, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Sample SD&quot;</span>, <span class="at">y =</span> <span class="cn">NULL</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="fu">ggsave</span>(<span class="st">&quot;sds_w_bessel.png&quot;</span>, plot, <span class="at">dpi =</span> <span class="dv">300</span>, <span class="at">width =</span> <span class="dv">4</span>, <span class="at">height =</span> <span class="dv">4</span>, <span class="at">units =</span> <span class="st">&quot;in&quot;</span>)</span></code></pre></div>
</details>
<p><img style="width:50%;" src="sds_w_bessel.png" /></p>
<p>This helps quite a bit (though it isn’t perfect).</p>
<p>For leading and didactic reasons, let’s also look at the
distributions of the sample variances without and with Bessel’s
correction:</p>
<pre><code>vars &lt;- sapply(samples, calc_var)

plot &lt;- ggplot(data.frame(x = vars), aes(x)) +
  theme_kai() +
  geom_histogram(bins = 1000) +
  geom_vline(xintercept = sd^2, color = ACCENT) +
  geom_vline(xintercept = mean(vars), color = ACCENT, linetype = 2) +
  labs(x = &quot;Sample Variance&quot;, y = NULL)
ggsave(&quot;vars.png&quot;, plot, dpi = 300, width = 4, height = 4, units = &quot;in&quot;)</code></pre>
<figure>
<img src="./vars.png" alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<pre><code>vars &lt;- sapply(samples, var)

plot &lt;- ggplot(data.frame(x = vars), aes(x)) +
  theme_kai() +
  geom_histogram(bins = 1000) +
  geom_vline(xintercept = sd^2, color = ACCENT) +
  geom_vline(xintercept = mean(vars), color = ACCENT, linetype = 2) +
  labs(x = &quot;Sample Variance&quot;, y = NULL)
ggsave(&quot;vars_w_bessel.png&quot;, plot, dpi = 300, width = 4, height = 4, units = &quot;in&quot;)</code></pre>
<figure>
<img src="./vars_w_bessel.png" alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>You’ll notice that while variance was consistently underestimated
before Bessel’s correction, it lined up exactly after we used Bessel’s
correction. I’ll bring this up in more detail later, but for now I think
we are forced to agree that Bessel’s correction is doing
<em>something</em> good, and is worth investigating further.</p>
<p><a id="orgfeb3aa8"></a></p>
<h1 id="explanation-ii-it-accounts-for-reusing-data">Explanation II: It
accounts for reusing data</h1>
<p>Besides the explanation of ‘it just does work, do it’ (which is,
admittedly, not an explanation but rather a motivation), we can explain
Bessel’s correction in terms of combating ‘data-reuse’.</p>
<p>When calculating the standard deviation, we find the differences
between the mean and a given value. No worries if we know the mean of
the population, but in my forays with statistics this is an incredibly
rare scenario. Much more likely is that we are forced to use the mean of
the <em>sample</em>, then calculate the standard deviation using the
differences of the values of the sample and the mean of the sample.</p>
<p>As we explored a bit in <a
href="https://kai.rbind.io/posts/2024-09-22_deviation-deep-dive-pt1/">the
first blog post of this series</a>, variance is defined in large part by
the distances of the values from the mean, usually either the sample
mean or the population mean. As it turns out, the variance is minimized
when that value is the mean of the sample.</p>
<p><a id="org3a847bb"></a></p>
<h2 id="wait-i-dont-believe-you">Wait, I don’t believe you</h2>
<p>That’s fine. I didn’t believe it either. But it turns out that some
fairly harmless algebra can be used to show this is true.</p>
<p>(NB: This section is <em>optional</em>. If you choose to believe me
(foolish) that the point at which to take distances from for variance is
minimized by using the sample mean, you can move to the next section.
Otherwise, read on.)</p>
<p>The definition of variance is:</p>
<p><span class="math display">\[\begin{align}
Var(X) := E\big[(X-\bar{X})^2\big]
\end{align}\]</span></p>
<p>That is, variance is the average of the squared differences between
the random variable <span class="math inline">\(X\)</span> and its mean
(aka expectation, <span class="math inline">\(E[X]\)</span>, denoted by
<span class="math inline">\(\bar{X}\)</span>).</p>
<p>One way to determine if <span class="math inline">\(\bar{X}\)</span>
will result in the smallest <span class="math inline">\(Var(X)\)</span>
is to assume that maybe some other constant (let’s call it <span
class="math inline">\(c\)</span>) would be better. That is, what is
this:</p>
<p><span class="math display">\[\begin{align}
E\big[(X-c)^2\big]
\end{align}\]</span></p>
<p>One clever trick we can do to get this in terms of the variance
definition we know and love is to add and subtract <span
class="math inline">\(\bar{X}\)</span> to this equation (effectively
adding 0, maintaining equality):</p>
<p><span class="math display">\[\begin{align*}
\newcommand\xb{\bar{X}}
\newcommand\a{\textcolor{red}{(X-\xb)}}
\newcommand\b{\textcolor{blue}{(\xb - c)}}
E\big[(X-c)^2\big] &amp;= E\big[(X \textcolor{#f7d060}{ - \xb + \xb} -
c)^2\big]\\
&amp;= E\Big[\big(\a + \b\big)^2\Big]\\
&amp;= E\big[\a^2 + 2\a\b + \b^2\big] \\
&amp;\overset{1}= E[\a^2] + E[2\a\b] + E[\b^2]
\end{align*}\]</span></p>
<p>Since <span class="math inline">\(\bar{X}\)</span> and <span
class="math inline">\(c\)</span> are constants, <span
class="math inline">\(E[\bar{X}]\)</span>, <span
class="math inline">\(E[c]\)</span>, and <span
class="math inline">\(E[\bar{X} - c]\)</span> will be just be <span
class="math inline">\(\bar{X}\)</span>, <span
class="math inline">\(c\)</span>, and <span
class="math inline">\(\bar{X}-c\)</span></p>
<p><span class="math display">\[\begin{align*}
\newcommand\xb{\bar{X}}
\newcommand\a{(X-\xb)}
\newcommand\b{(\xb - c)}
\newcommand{\hl}[1]{{\color{goldenrod}{#1}}}

E\big[(X-c)^2\big] &amp;= E[\a^2] + E[\hl{2}\a\hl{\b}] + E[\hl{\b^2}]\\
&amp;= E[\a^2] + 2\b(E[X - \hl{\xb}]) + \b^2\\
&amp;\overset{1}= E[\a^2] + 2\b(E[X] - \xb) + \b^2
\end{align*}\]</span></p>
<p>Also, since <span class="math inline">\(E[X] := \bar{X}\)</span>,</p>
<p><span class="math display">\[\begin{align*}
\newcommand\xb{\bar{X}}
\newcommand\a{(X-\xb)}
\newcommand\b{(\xb - c)}
E\big[(X-c)^2\big] &amp;= E[\a^2] + 2\b(\hl{\xb} - \xb) + \b^2\\
&amp;= E[\a^2] + \hl{0} + \b^2\\
&amp;= E[\a^2] + \b^2
\end{align*}\]</span></p>
<p>We can see that the first term is our original definition of
variance. The second term is squared and therefore can only be positive.
At best, it can be 0, which only happens when <span
class="math inline">\(c = \bar{X}\)</span>. Therefore, variance is
minimized when the thing we’re measuring distances from is the mean of
<span class="math inline">\(X\)</span>.</p>
<p><a id="orgb091e26"></a></p>
<h2 id="okay-i-believe-you">Okay, I believe you</h2>
<p>Since you either blindly believe me or have been convinced that
measuring all the distances from the sample mean results in the smallest
variance, we can now consider the implications.</p>
<p>For any given sample, the sample mean will almost never be the
population mean. If the sample mean will always give the lowest
variance, the population mean (the thing we would use to calculate
variance if we knew it because it gives us the correct result) will
always give a higher variance (except in the case in which it is exactly
equal, which is unlikely).</p>
<p>Intuitively, this has been described as painting a target around a
bunch of bullet holes, rather than shooting at a target. You’re peeking
at the future and then wondering why you’re correct all the time.</p>
<p>This is usually where the concept of ‘degrees of freedom’ gets
invoked. If someone tells you that you can pick any 5 numbers, so long
as they have a mean of, say, 20, you’re free to do whatever you want to
do with 4 of them - but that last one has to pick up the bill to get the
mean to be 20. That is, there are only 4 values that are ‘free to vary’
- only 4 <em>degrees of freedom</em>.</p>
<p>This is true for an arbitrary mean and an arbitrary number of values
- you will always have <span class="math inline">\(n-1\)</span> degrees
of freedom if you are calculating a new statistic reusing old
information (in the case of variance, the sample mean isn’t truly free
to vary: it is bound by the sample). So it’s kind of like instead of
having <span class="math inline">\(n\)</span> values, you have <span
class="math inline">\(n-1\)</span> values. Dividing by this inflates the
variance a bit to combat the artificial deflation, and all is well in
the world. Right?</p>
<p><a id="org601cde4"></a></p>
<h1 id="explanation-iii-for-those-suspicious-of-simplicity">Explanation
III: For those suspicious of simplicity</h1>
<p><span class="math inline">\(n - 1\)</span> feels pretty dang neat.
<em>Too</em> neat. To rigorously prove that dividing by <span
class="math inline">\(n-1\)</span> rather that <span
class="math inline">\(n\)</span> provides the correction we need, we can
pay our dues with math.</p>
<p>Gregory Gundersen (who has a <a
href="https://gregorygundersen.com/">very nice blog</a>) has a <a
href="https://gregorygundersen.com/blog/2019/01/11/bessel/">post</a>
proving Bessel’s correction that I’m largely going to follow, expounding
on points I found confusing (not due to Gregory but rather my own
knowledge gaps. I also tend to write the individual steps out more to
hopefully reduce your cognitive overhead).</p>
<p>The sample variance <span class="math inline">\(s^2\)</span> is
defined by</p>
<p><span class="math display">\[
s^2 := \frac{1}{n}\sum^n_{i=1}(X_i - \bar{X})^2
\]</span></p>
<p>Note that compared to <span class="math inline">\(\sigma^2\)</span>
(the population variance), <span class="math inline">\(s^2\)</span>
measures the difference between values and the sample mean (<span
class="math inline">\(\bar{X}\)</span>) rather than the population mean
(<span class="math inline">\(\mu\)</span>).</p>
<p>Our goal, ultimately, is to find <span
class="math inline">\(E[s^2]\)</span> in terms of <span
class="math inline">\(\sigma^2\)</span> and then see by how much it
differs, and if that difference corresponds with Bessel’s
correction.</p>
<p><span class="math display">\[\begin{align*}
\newcommand\sm{\sum^n_{i=1}}
\newcommand\fr{\frac{1}{n}}
\newcommand\xb{\bar{X}}
\newcommand{\hl}[1]{{\color{goldenrod}{#1}}}
E[s^2] &amp;= E[\fr \sm(X_i - \xb)^2]\\
&amp;\overset{1}= \hl{\fr} E[\sm(X_i - \xb)^2]\\
&amp;= \fr E[\sm X_i^2 + \sm(-2X_i\xb) + \sm \xb^2]\\
&amp;= \fr E[\sm X_i^2 - 2\xb\sm X_i + n\xb^2]\\
&amp;= \fr E[\sm X_i^2 - 2\xb^2 + n\xb^2]\\
&amp;= \fr E[\sm X_i^2 - n\xb^2]\\
&amp;\overset{1}= \fr \Big(E[\sm X_i^2] - E[n\xb^2]\Big)\\
&amp;\overset{1}= \fr \Big(E[\sm X_i^2] - \hl{n}E[\xb^2]\Big)\\
&amp;= \fr E[\sm X_i^2] - E[\xb^2]\\
&amp;\overset{1}= \fr \sm E[X_i^2] - E[\xb^2]\\
&amp;= \textcolor{red}{E[X_i^2]} - \textcolor{blue}{E[\xb^2]}\\
\end{align*}\]</span></p>
<p>Since</p>
<p><span class="math display">\[\begin{align*}
Var(X) &amp;\overset{2.1}= E[X^2]- E[X]^2\\
\implies E[X^2] &amp;= Var(X) + E[X]^2
\end{align*}\]</span></p>
<p>Substituting <span class="math inline">\(X\)</span> for <span
class="math inline">\(X_i\)</span> gets us</p>
<p><span class="math display">\[\begin{align*}
\color{red}{E[X_i^2]} \color{white}{= Var(X_i) + E[X_i]^2}
\end{align*}\]</span></p>
<p><span class="math inline">\(Var(X_i) = \sigma^2\)</span> and <span
class="math inline">\(E[X_i] = \mu\)</span> (note that <span
class="math inline">\(X_i\)</span> is not a value but rather the process
of drawing a single value, so this equality still holds), so</p>
<p><span class="math display">\[\begin{align*}
\textcolor{red}{E[X_i^2]} &amp;= Var(X_i) + E[X_i]^2 \\
&amp;= \sigma^2 + \mu^2
\end{align*}\]</span></p>
<p>Using the same equation above but substituting <span
class="math inline">\(\bar{X}\)</span> for <span
class="math inline">\(X\)</span> instead of <span
class="math inline">\(X_i\)</span> for <span
class="math inline">\(X\)</span>, we get:</p>
<p><span class="math display">\[\begin{align*}
\textcolor{blue}{E[\bar{X}^2]} = \textcolor{green}{Var(\bar{X})} +
E[\bar{X}]^2
\end{align*}\]</span></p>
<p><span class="math inline">\(Var(\bar{X})\)</span> is a bit different
than <span class="math inline">\(Var(X_i)\)</span>, but can be
simplified like so:</p>
<p><span class="math display">\[\begin{align*}
\textcolor{green}{Var(\bar{X})} &amp;= Var(\frac{1}{n}\sum^n_{i=1}X_i)
\\
&amp;\overset{2.2}= \frac{1}{n^2}Var(\sum^n_{i=1}X_i) \\
&amp;\overset{2.3}= \frac{1}{n^2}\sum^n_{i=1}Var(X_i) \\
&amp;= \frac{1}{n^2}\sum^n_{i=1}\sigma^2 \\
&amp;= \frac{n\sigma^2}{n^2} \\
&amp;= \frac{\sigma^2}{n}
\end{align*}\]</span></p>
<p>Plugging this back into the equation, we get:</p>
<p><span class="math display">\[\begin{align*}
\textcolor{blue}{E[\bar{X}^2]} &amp;= \textcolor{green}{Var(\bar{X})} +
E[\bar{X}]^2 \\
&amp;= \textcolor{green}{\frac{\sigma^2}{n}} + E[\bar{X}]^2 \\
&amp;= \frac{\sigma^2}{n} + \mu^2 \\
\end{align*}\]</span></p>
<p>So, substituting these for <span
class="math inline">\(E[X_i^2]\)</span> and <span
class="math inline">\(E[\bar{X}^2]\)</span>, we get</p>
<p><span class="math display">\[\begin{align*}
E[s^2] &amp;= \textcolor{red}{E[X_i^2]} - \textcolor{blue}{E[\bar{X}^2]}
\\
&amp;= \textcolor{red}{(\sigma^2 + \mu^2)} -
\textcolor{blue}{(\frac{\sigma^2}{n} + \mu^2)} \\
&amp;= \sigma^2 - \frac{\sigma^2}{n} \\
&amp;= \sigma^2 (1-\frac{1}{n}) \\
&amp;= \sigma^2 \frac{n-1}{n}
\end{align*}\]</span></p>
<p>Rearranging this equation, we get:</p>
<p><span class="math display">\[\begin{align}
E[s^2] &amp;= \sigma^2 \frac{n-1}{n} \implies \\
\sigma^2 &amp;= \frac{n}{n-1}E[s^2]
\end{align}\]</span></p>
<p>That is to say, rather than divide by <span
class="math inline">\(n\)</span> (undo dividing by <span
class="math inline">\(n\)</span> by multiplying it) divide instead by
<span class="math inline">\(n-1\)</span>. This is Bessel’s correction.
Was it worth it??? All the suffering????</p>
<p><a id="orgec179fc"></a></p>
<h1 id="do-we-need-bessels-correction-at-all">Do we need Bessel’s
correction at all?</h1>
<p>‘It depends’. Of course it does. Bessel’s correction removes bias
(that is, the difference between the true statistic and the estimated
statistic) for variance, so in the case where theoretical perfection is
required, this might be preferred. Indeed, the theory behind it is
pretty interesting! However, it doesn’t completely remove bias in the
case of standard deviation (as we saw in some of our first plots), and
it’s frankly confusing as hell. Jeffery Rosenthal wrote a <a
href="https://imstat.org/2015/11/17/the-kids-are-alright-divide-by-n-when-estimating-variance/">beautiful
article</a> summarizing the challenges of teaching Bessel’s correction,
and mentioned that the smallest mean-squared-error results not from
dividing by <span class="math inline">\(n-1\)</span>, or even <span
class="math inline">\(n\)</span>, but <span
class="math inline">\(n+1\)</span>. As such, it might be best to leave
Bessel’s correction as a theoretical curiosity for more advanced
studies, stick with dividing by <span class="math inline">\(n\)</span>
for didactic purposes, and <span class="math inline">\(n+1\)</span> for
applications that require the smallest MSE. However, I hope this post at
least helps explain the why and how <span
class="math inline">\(n-1\)</span> came to be in the first place, since
it is still very much present.</p>
<p><a id="org8d7cd77"></a></p>
<h1 id="appendix">Appendix</h1>
<p><a id="org064f357"></a></p>
<h2 id="random-variable">Random Variable</h2>
<p>A random variable isn’t really random nor is it really a variable.
There are a lot of people smarter than me that would quibble with this
definition, but a random variable is probably better described as a
function that takes some result of an event (a roll of a die, a flip of
a coin, a heart rates measurement) and turns it into a number (in the
case of a die roll and heart measurement, the conversion is pretty
obvious. For heads and tails, one of them is probably going to be 1, one
is probably going to be 0, and the random variable does the
conversion).</p>
<p>Just like a function doesn’t have a value until called with an
argument, a random variable doesn’t have a value until it is
realized.</p>
<p><a id="org48f5bf6"></a></p>
<h2 id="linearity-of-expectation">1 Linearity of Expectation</h2>
<p>Expectation (which for our purposes is identical to the mean) of a
random variable has the following definition:</p>
<p><span class="math display">\[\begin{equation}
E[X] = \frac{1}{n}\sum^{n}_{i = 1}X_i
\end{equation}\]</span></p>
<p>It has some algebraically useful properties, namely:</p>
<p><span class="math display">\[\begin{equation}
E[aX] = aE[X]
\end{equation}\]</span></p>
<p>where <span class="math inline">\(a\)</span> is some constant,
and</p>
<p><span class="math display">\[\begin{equation}
E[X + Y] = E[X] + E[Y]
\end{equation}\]</span></p>
<p>An extension of this is that</p>
<p><span class="math display">\[\begin{equation}
E[\Sigma X] = \Sigma E[X]
\end{equation}\]</span></p>
<p><a id="orgcedb94a"></a></p>
<h3 id="proof">Proof</h3>
<p>We can show this using the definition of the mean:</p>
<p><span class="math display">\[\begin{align*}
E[aX] &amp;= \frac{1}{n}\sum^{n}_{i = 1}aX_i\\
&amp;= \frac{1}{n}(aX_1 + aX_2 + ... + aX_n)\\
&amp;= a\frac{1}{n}(X_1 + X_2 + ... + X_n)\\
&amp;= aE[X]
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
E[X + Y] &amp;= \frac{1}{n}\sum^{n}_{i = 1}(X_i + Y_i)\\
&amp;= \frac{1}{n}(X_1 + Y_1 + ... + X_n + Y_n)\\
&amp;= \frac{1}{n}[(X_1 + ... + X_n) + (Y_1 + ... + Y_n)] \\
&amp;= \frac{1}{n}(X_1 + ... + X_n) + \frac{1}{n}(Y_1 + ... + Y_n) \\
&amp;= E[X] + E[Y]
\end{align*}\]</span></p>
<p><a id="org72d96be"></a></p>
<h2 id="variance">2 Variance</h2>
<p><a id="org68ae905"></a></p>
<h3 id="rearrangement">2.1 Rearrangement</h3>
<p>Turns out, <span class="math display">\[
Var(X) = E[(X-\bar{X})^2] = E[X^2]- (E[X])^2
\]</span></p>
<ol type="1">
<li><p>Proof</p>
<p><span class="math display">\[\begin{align*}
Var(X) &amp;= E[(X-\bar{X})^2] \\
&amp;= E[X^2- 2X\bar{X} + \bar{X}^2] \\
&amp;= E[X^2]- E[2X\bar{X}] + E[\bar{X}^2] \\
&amp;= E[X^2]- 2\bar{X}E[X] + \bar{X}^2 \\
&amp;= E[X^2]- 2\bar{X}^2 + \bar{X}^2 \\
&amp;= E[X^2]- \bar{X}^2 \\
&amp;= E[X^2]- (E[X])^2 \\
\end{align*}\]</span></p></li>
</ol>
<p><a id="orgb9c3ba4"></a></p>
<h3 id="varax-a2-varx">2.2 Var(aX) = a<sup>2</sup> Var(X)</h3>
<p>Turns out, <span class="math display">\[
Var(aX) = a^2Var(X)
\]</span></p>
<p>where <span class="math inline">\(a\)</span> is a constant.</p>
<ol type="1">
<li><p>Proof</p>
<p>Using the rearranged formula from the previous section and
substituting <span class="math inline">\(aX\)</span> for everywhere we
see <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[\begin{align*}
Var(aX) &amp;= E[(aX)^2]- (E[aX])^2 \\
&amp;= E[a^2X^2]- (E[aX])^2 \\
&amp;= a^2E[X^2]- (aE[X])^2 \\
&amp;= a^2E[X^2]- a^2(E[X])^2 \\
&amp;= a^2(E[X^2]- (E[X])^2) \\
&amp;= a^2Var(X)
\end{align*}\]</span></p></li>
</ol>
<p><a id="orgd57f94a"></a></p>
<h3 id="bienaymés-formula">2.3 Bienaymé’s formula</h3>
<p>Another algebraically useful property is that if the values of a
random variable are independent, then the sum of the variance is equal
to the variance of the sum. Or rather:</p>
<p><span class="math display">\[\begin{equation}
Var(\sum^n_{i=1}X_i) = \sum^n_{k=1}(Var(X_k))
\end{equation}\]</span></p>
<ol type="1">
<li><p>Proof</p>
<p>The following proof is taken from <a
href="https://stats.stackexchange.com/questions/31177/does-the-variance-of-a-sum-equal-the-sum-of-the-variances">here</a>.</p>
<p>Using the proof of the previous section that</p>
<p><span class="math display">\[\begin{align*}
Var(X) = E[X^2]- (E[X])^2
\end{align*}\]</span></p>
<p>Apparently, the <a
href="https://math.stackexchange.com/questions/3182767/proof-of-sum-of-random-variables-is-a-random-variable">sum
of random variables is also a random variable</a>, so this holds
for:</p>
<p><span class="math display">\[\begin{align*}
Var(\sum^n_{i=1}X_i) = E[(\sum^n_{i=1}X_i )^2]- (E[\sum^n_{i=1}X_i])^2
\end{align*}\]</span></p>
<p>In the linked answer, they helpfully note that</p>
<p><span class="math display">\[\begin{align*}
(\sum^n_{i=1}X_i)^2 = \sum^n_{i=1}\sum^n_{j=1}X_i X_j
\end{align*}\]</span></p>
<p>A beautiful explanation they make is, quote:</p>
<blockquote>
<p>which is clear if you think about what you’re doing when you
calculate
(X<sub>1</sub>+…+X<sub>n</sub>)⋅(X<sub>1</sub>+…+X<sub>n</sub>) by
hand.</p>
</blockquote>
<p>So,</p>
<p><span class="math display">\[\begin{align*}
Var(\sum^n_{i=1}(X_i)) &amp;= E[(\sum^n_{i=1}X_i )^2]-
(E[\sum^n_{i=1}X_i])^2 \\
&amp;= E[\sum^n_{i=1}\sum^n_{j=1}X_i X_j] - (E[\sum^n_{i=1}X_i])^2 \\
&amp;\overset{1.1}= \sum^n_{i=1}\sum^n_{j=1}E[X_i X_j] -
(E[\sum^n_{i=1}X_i])^2 \\
&amp;\overset{1.1}= \sum^n_{i=1}\sum^n_{j=1}E[X_i X_j] -
(\sum^n_{i=1}E[X_i])^2 \\
&amp;= \sum^n_{i=1}\sum^n_{j=1}E[X_i X_j] -
\sum^n_{i=1}\sum^n_{j=1}E[X_i]E[X_j]\\
&amp;= \sum^n_{i=1}\sum^n_{j=1}(E[X_i X_j] - E[X_i]E[X_j]) =
\sum^n_{i=1}\sum^n_{j=1}Cov(X_i,X_j)\\
\end{align*}\]</span></p>
<p>In the case of independent sampling, we expect covariance to be 0
between any non-identical random variables (in other words, if we
thought about it like a matrix we would expect 0s everywhere not on the
diagonal), so:</p>
<p><span class="math display">\[\begin{align*}
\sum^n_{i=1}\sum^n_{j=1}Cov(X_i,X_j) = \sum^n_{i=1}Cov(X_i, X_i)
\end{align*}\]</span></p>
<p>If we look at the rearranged definition of covariance</p>
<p><span class="math display">\[
Cov(X,Y) = E[XY] - E[X]E[Y]
\]</span></p>
<p>and consider the case in which <span class="math inline">\(X =
Y\)</span>:</p>
<p><span class="math display">\[
Cov(X,X) = E[XX] - E[X]E[X]
\]</span></p>
<p>We realize that this is just the definition of variance:</p>
<p><span class="math display">\[
Cov(X,X) = E[XX] - E[X]E[X] = E[X^2] - E[X]^2 = Var(X)
\]</span></p>
<p>So, substituting this back in, we see:</p>
<p><span class="math display">\[\begin{align*}
Var(\sum^n_{i=1}X_i) &amp;= \sum^n_{i=1}Cov(X_i,X_i)\\
&amp;= \sum^n_{i=1}Var(X_i)
\end{align*}\]</span></p></li>
</ol>
<p><a id="org255e62a"></a></p>
<h2 id="covariance">Covariance</h2>
<p>Covariance can be defined as follows:</p>
<p><span class="math display">\[
Cov(X,Y) = E[(X - E[X])(Y-E[Y])]
\]</span></p>
<p>Which can be rewritten, using the linearity of expectation, as:</p>
<p><span class="math display">\[
Cov(X,Y) = E[XY] - E[X]E[Y]
\]</span></p>
<p><a id="org29834ff"></a></p>
<h1 id="sources">Sources</h1>
<p>As always, Wikipedia served as a great entry citation for many
downstream citations:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Bessel%27s_correction"
class="uri">https://en.wikipedia.org/wiki/Bessel%27s_correction</a></li>
</ul>
<p>Explanation of why sample mean results in the lowest variance adapted
from:</p>
<ul>
<li><a href="https://stats.stackexchange.com/a/520328"
class="uri">https://stats.stackexchange.com/a/520328</a></li>
</ul>
<p>Andy Field’s “Discovering Statistics” book has a great explanation
about why <span class="math inline">\(n-1\)</span> using the
degrees-of-freedom argument</p>
<p>Nice proof of Bessel’s Correction:</p>
<ul>
<li><a href="https://gregorygundersen.com/blog/2019/01/11/bessel/"
class="uri">https://gregorygundersen.com/blog/2019/01/11/bessel/</a></li>
</ul>
<p>Why do constants get squared when in variance:</p>
<ul>
<li><a href="https://math.stackexchange.com/a/1708274"
class="uri">https://math.stackexchange.com/a/1708274</a></li>
</ul>
<p>Should we care about Bessel’s correction?</p>
<ul>
<li><a
href="https://imstat.org/2015/11/17/the-kids-are-alright-divide-by-n-when-estimating-variance/"
class="uri">https://imstat.org/2015/11/17/the-kids-are-alright-divide-by-n-when-estimating-variance/</a></li>
</ul>
<p>Nice proof of Bienaymé’s formula:</p>
<ul>
<li><a href="https://stats.stackexchange.com/a/31181"
class="uri">https://stats.stackexchange.com/a/31181</a></li>
</ul>

</body>
</html>
